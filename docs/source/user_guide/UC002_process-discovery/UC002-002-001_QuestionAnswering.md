# NLP каскад. Вопросно-ответная система (Question & Answering)

![Machine Learning](https://img.shields.io/badge/Machine%20Learning-indianred?style=for-the-badge) ![Exploratory data analysis](https://img.shields.io/badge/Exploratory%20data%20analysis-steelblue?style=for-the-badge) ![Pretrained](https://img.shields.io/badge/Pretrained-gray?style=for-the-badge)

> Модуль решает задачу `extractive Q&A` - поиска ответа в тексте на заданный вопрос. Языковая модель найдет часть текста, которая максимально близко отвечает на вопрос, и вернет ее пользователю.

`````{admonition} Идея для анализа бизнес-процесса
:class: tip

Текстовые атрибуты лога могут быть богаты на инсайты. Однако обычно они представлены в свободной форме и их обработка вручную затруднена. Кроме того в тексте могут упоминаться причины возникновения проблем, комментарии по положительным изменениям или описания решений проблем, и, если анализировать их вместе, не разделяя, можно придти к неправильным выводам. Подход Q&A позволяет задать вопрос к тексту и извлечь из него только необходимую информацию, то есть - ответ на заданный вопрос. С извлеченными ответами далее можно работать, как обычно - кластеризовать, выделить тематические направления либо построить карту слов.

`````

В рамках библиотеки данная модель реализована как модуль `sberpm.nlp.qa_extractor`.

## Формализация задачи для анализа процессов

Зачастую лог автоматизированной системы фиксирует обращения пользователя по возникающим в процессе проблемам, например, при подаче заявки. В случае, если обращения не типизированы, в логе обязательно присутствует колонка со свободным текстовым полем, которое не ограничено количеством допустимых символов или слов.

## Алгоритм в основе модуля

Языковая модель может быть особенно полезна в случае, когда текст содержит большое количество информации, а анализ (например, кластеризацию) необходимо провести только по какой-то смысловой части в нем. 


`````{admonition} Extractive Q&A
:class: info
Extractive Q&A - это класс задач, который предполагает выявление и извлечение ответа на вопрос непосредственно из текста без генерации. Языковая модель не имеет особенностей реализации для домена Process Mining, является универсальной.

`````

В таком случае, целесообразно применить модель на текстовую колонку несколько раз, задавая разные вопросы, чтобы появилась возможность анализировать каждый смысловой блок отдельно.
Полученные кластеры с каждого смыслового блока можно затем использовать в качестве факторов уже в новой кластеризации.

``` {list-table}  Пример использования Q&A
:header-rows: 1

*   - Текст обращения
    - Вопрос, который задаем для упрощения текста
    - Ответ модели Q&A
   
*   - Получаем некорректные данные от подразделения А, приходится перепроверять
    - С какой проблемой обратился пользователь?
    - некорректные данные

*   - Получаем некорректные данные от подразделения А, приходится перепроверять
    - Кто виноват?
    - подразделения А

*   - Сложно довериться данным, которые загружает менеджер Максим, их невозможно проверить
    - С какой проблемой обратился пользователь?
    - невозможно проверить

*   - Сложно довериться данным, которые загружает менеджер Максим, их невозможно проверить
    - Кто виноват?
    - менеджер Максим

```

Вопрос рекомендуется задавать четко и односложно, используя тот же стиль, в котором написан текст, не рекомендуется в одном вопросе использовать подвопросы и уточнения.

### Технический ландшафт

Впервые архитектура трансформеров была опубликована в июне 2017. Основной фокус оригинального исследования был сосредоточен на задачах перевода. Эта публикация повлекла за собой несколько влиятельных моделей: GPT (2018), BERT (2018), T5 (2019), GPT-3 (2020).

Все модели трансформеров, упомянутые выше (GPT, BERT, BART, T5, etc.) обучены как языковые модели (англ. language models). Это означает, что они обучены на огромном количестве текста, используя технику самостоятельного обучения (англ. self-supervised learning). Самостоятельное обучение - это такой способ обучения, в котором цель обучения автоматически вычисляется на основе входных данных - то есть люди не размечают данные.

Такой тип моделей реализует статистическое понимание языка, на котором он был обучен, но он не очень полезен для конкретных практических задач. Из-за этого базовая предварительно обученная модель потом подвергается процедуре, называемой трансферным обучением (англ. transfer learning). В ходе этого процесса модель настраивается под конкретные наблюдения, т.е. размеченными человеком данными для конкретной задачи.

Подробнее про архитектуры трансформеров можно прочитать [здесь](https://huggingface.co/learn/nlp-course/ru/chapter1/4).

Для решения задачи Q&A модель вида трансформер проходит процедуру дообучения на данных вида "Вопрос" (Question) - "Ответ" (Answer), отчего задача и получила такое название. Важно отметить, что в качестве ответа указывается конкретная позиция начала и окончания из исходного текста, то есть решается задача извлечения, а не генерации текста (англ. extraction).

Рекомендованная архитектура модели: [mDeBERTaV3: Improving DeBERTa using ELCTRA pre-training with gradient disentangled embedding sharing](https://arxiv.org/pdf/2111.09543v4).

### Модификация параметров алгоритма

``` {list-table} Параметры модели
:header-rows: 1

*   - Параметр
    - Интерпретация 
   
*   - {bdg-primary-line}`Вопрос`
    - Вопрос в свободной форме. Четкий, предполагающий односложный ответ.
      
*   - {bdg-dark-line}`Название текстовой колонки`
    - Поскольку используемая модель является контекстной, необходимо, чтобы текст в указанной колонке был похож на предложение-повествование. Предложений может быть несколько, но ожидается, что они связаны по смыслу. С перечислениями слов и фраз модель тоже работает, но значительно хуже.  

*   - {bdg-dark-line}`Путь к модели`
    - К использованию подходят любые модели, имеющие архитектуру упомянутого типа. Рекомендуется использовать mdeberta-v3.
```

### Краевые случаи и ограничения

Рекомендуется:

* выполнять тестовый прогон для сформулированного вопроса на небольшой части датасета, чтобы убедиться, что его формулировка корректна и модель понимает его правильно;
* если необходимо - переформулируйте вопрос, до того как выполните запуск на полном датасете;

Стоит иметь в виду, что:

* модель возвращает ответ ровно так, как в исходном тексте, не изменяя форму слова;
* модель возвращается только одно подходящее вхождение (слово или набор слов), и, если далее по тексту встречается фраза, которая также может являться ответом, в выдачу она уже не попадёт. Например: при вопросе "Кто обработал заявки?" к тексту "Менеджер Иванов, Петров обработали 100 заявок. А менеджер Марина 10", модель может ответить так: "Менеджер Иванов, Петров", либо так: "менеджер Марина";
* модель всегда возвращает ответ (иначе говоря - не может не ответить), в связи с чем ответы иногда могут быть не релевантны.
* языковая модель, которую использует модуль, относится к контекстным, то есть умеет работать с синонимами, антонимами и так далее (другими словами - опирается на контекст). Такие модели хуже работают с канцеляризмами, жаргонизмами, большим количеством аббревиатур и специальных терминов;
* при использовании маловесных языковых моделей внутри модуля ответы ухудшаются, поскольку такие модели "знают мир хуже" (обучены на малых данных, "видели" меньше слов и их употреблений);
* при использовании маловесных языковых моделей могут возникнуть сложности с длинными текстами, поскольку такие модели "не имеют памяти" (не хранят достаточно информации, что упоминалось в тексте ранее, поэтому теряется смысл);
* при использовании тяжелых языковых моделей внутри модуля время обработки и потребляемая память значительно увеличиваются.
